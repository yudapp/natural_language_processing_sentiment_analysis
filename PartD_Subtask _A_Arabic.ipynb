{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: A Real Sentiment Analysis Task: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate a Basic Classifier for Subtask A in Arabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Process the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the path to the folder containing the text files. We read each text file in the folder. We append the tweets one by one in the \"raw_data\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "from os.path import isfile\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Arabic stemmer downloaded from https://www.arabicstemmer.com/python/\n",
    "from snowballstemmer import stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple file reader without exception handling\n",
    "def readFile(path):\n",
    "    files=filter(isfile,glob.glob('%s/*'%path))\n",
    "    raw_data =[]\n",
    "    for name in files:\n",
    "        with open(name, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                record = line.strip().split() \n",
    "                raw_data.append(record)\n",
    "    \n",
    "    return raw_data # return the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'S:\\IT\\Queen Mary\\NLP\\Assignment 1\\Arabic\\train'\n",
    "raw_train = readFile(path)\n",
    "\n",
    "#for the test set\n",
    "path = r'S:\\IT\\Queen Mary\\NLP\\Assignment 1\\Arabic\\test'\n",
    "raw_test = readFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:  ['783677250768138240', 'neutral', '\"RT', '@RotanaMagz:', 'بين', '#نوت7', 'و', '#آيفون7', 'عنصر', 'مشترك!!https://t.co/WY32OzQFve#روتانا', '#تكنولوجيا\"']\n"
     ]
    }
   ],
   "source": [
    "print(\"raw data: \", raw_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process the file\n",
    "def processData(raw_data):\n",
    "    ar_stemmer = stemmer(\"arabic\")\n",
    "    stop_list = stopwords.words('arabic') + list(string.punctuation)\n",
    "    normalized_data = []\n",
    "    labels = []\n",
    "    special = \"\\\\u\" # unicode we want to eliminate but keep the word associated with it\n",
    "    for i in range(len(raw_data)):\n",
    "        result = [] # clears result after each sentence\n",
    "        for w in raw_data[i]:\n",
    "            if special in w:\n",
    "                idx = w.index(special)\n",
    "                word = w[:idx] #take only the first part of the word and ignore 'special'\n",
    "            else:\n",
    "                word = w # otherwise word stays the same\n",
    "            word = word.lower() #change word to lower case\n",
    "\n",
    "            if word not in stop_list and word.isalpha() and len(word) > 1: #filter out punctuations,stop words and digits\n",
    "                word = ar_stemmer.stemWord(word) #apply lemmatizer and skip label(positive, etc..)\n",
    "                result.append(word) #append each word to a phrase\n",
    "        if len(result) > 3:\n",
    "            normalized_data.append(result[1:]) #append each phrase to the file\n",
    "            lab = ''.join(result[:1])\n",
    "            if lab ==\"positive\":\n",
    "                labels.append(2)\n",
    "            elif lab ==\"negative\":\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "    return normalized_data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the training/validation set\n",
    "train_processed,train_labels = processData(raw_train)\n",
    "test_processed,test_labels = processData(raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train data:  ['اعلن', 'جوجل', 'يوم', 'ثلاثاء', 'هاتف', 'ذك', 'جديد', 'باسم', 'جهاز', 'واقع'] 0\n",
      "Example test data:  ['فعل', 'اتفق', 'اعلام', 'اكبر', 'مهجج', 'تعصب'] 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Example train data: \", train_processed[7], train_labels[7])\n",
    "print(\"Example test data: \", test_processed[6], test_labels[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find the top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate number of unique words and maximum length of phrases\n",
    "my_list = []\n",
    "mxlen = 0\n",
    "all_data = train_processed+test_processed\n",
    "for i in range(len(all_data)):\n",
    "    for s in all_data[i]:\n",
    "        if len(all_data[i]) > mxlen:\n",
    "            mxlen = len(all_data[i])\n",
    "        if s not in my_list:\n",
    "            my_list.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words found:  15293\n",
      "Maximum length of phrase found:  24\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words found: \", len(my_list))\n",
    "print(\"Maximum length of phrase found: \", mxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to put all the words together.\n",
    "# we use a vocabulary size of 700 which is around half of the number of unique words found\n",
    "VOCAB_SIZE = 7000\n",
    "INDEX_FROM = 2\n",
    "#put all the words together\n",
    "def wordsList(normalized_data):\n",
    "    longsen = []\n",
    "    for i in range(len(normalized_data)):\n",
    "        sen = normalized_data[i]\n",
    "        for w in sen:\n",
    "            longsen.append(w) #longsen cotains all the words in a list\n",
    "    return topWordsList(longsen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the top occuring words\n",
    "def topWordsList(longsen):\n",
    "    top_words = []\n",
    "    INDEX_FROM = 2\n",
    "    count = 0\n",
    "    fdist = nltk.FreqDist(longsen)\n",
    "    for word, frequency in fdist.most_common():\n",
    "        #print(u'{};{}'.format(word, frequency))\n",
    "        if count < (VOCAB_SIZE - INDEX_FROM):\n",
    "            count += 1\n",
    "            top_words.append(word)\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words = wordsList(train_processed+test_processed) # get a list of our vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill UNK and START tags in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with training data\n",
    "def tagList(my_data,topwords):\n",
    "    tagged_data = []\n",
    "    for i in range(len(my_data)):\n",
    "        out = []\n",
    "        out.append(\"<START>\")\n",
    "        for w in my_data[i]:\n",
    "            if w not in topwords:\n",
    "                w = \"<UNK>\"\n",
    "            else:\n",
    "                w = w\n",
    "            out.append(w) # one sentence\n",
    "        tagged_data.append(out) # the whole file\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_train = tagList(train_processed,vocab_words)\n",
    "tagged_test = tagList(test_processed,vocab_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of tagged data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged train data: ['<START>', 'عاص', 'حلان', 'ضيح', 'ابل', 'حيا', 'اولاد', 'ادفع', 'مقابل', 'اثار', 'مقال', 'نشر'] , label:  1\n",
      "Tagged test data: ['<START>', 'فعل', 'اتفق', 'اعلام', 'اكبر', '<UNK>', 'تعصب'] , label:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged train data:\",tagged_train[6],\", label: \",train_labels[6])\n",
    "print(\"Tagged test data:\",tagged_test[6], \", label: \", test_labels[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToIndex(data):\n",
    "    w2idx = {} # empty dictionary\n",
    "    w2idx [\"<PAD>\"] = 0\n",
    "    w2idx [\"<START>\"] = 1\n",
    "    w2idx [\"<UNK>\"] = 2  \n",
    "    counter = 3\n",
    "    for i in range(len(data)):\n",
    "        for w in data[i]:\n",
    "            if w not in w2idx.keys():\n",
    "                w2idx[w] = counter\n",
    "                counter = counter + 1\n",
    "    return w2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = wordToIndex(tagged_train+tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse the dictionary for index to word\n",
    "idx2word = dict([[v,k] for k,v in word2idx.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert sentences as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents as ids\n",
    "def sentID(datFile):\n",
    "    sents_as_ids = []\n",
    "    for i in range(len(datFile)):\n",
    "        converted = [] # empty list for every new line in corpus\n",
    "        for word in datFile[i]:\n",
    "            converted.append(word2idx[word]) #create sentence as a list of ids\n",
    "        sents_as_ids.append(converted)\n",
    "    return sents_as_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  7001\n",
      "\n",
      "Sample word2idx:  [('<PAD>', 0), ('<START>', 1), ('<UNK>', 2), ('ابل', 3), ('تعا', 4), ('فك', 5), ('شفر', 6), ('اجهز', 7), ('تتحدي', 8), ('اماز', 9), ('تنافس', 10), ('سامسونج', 11), ('هاتف', 12), ('جديد', 13), ('شرك', 14), ('واقع', 15), ('معزز', 16), ('يصبح', 17), ('اهم', 18), ('اسواق', 19)]\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words: ', len(word2idx))\n",
    "print('\\nSample word2idx: ', list(word2idx.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentences to ids\n",
    "train_data = sentID(tagged_train)\n",
    "test_data = sentID(tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: ['<START>', 'عاص', 'حلان', 'ضيح', 'ابل', 'حيا', 'اولاد', 'ادفع', 'مقابل', 'اثار', 'مقال', 'نشر']\n",
      "Training data sentences as ids:  [1, 30, 31, 32, 3, 33, 34, 35, 36, 37, 38, 39]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\",tagged_train[6])\n",
    "print(\"Training data sentences as ids: \",train_data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum length of a sentnce\n",
    "allData = train_data + test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = mxlen\n",
    "X_train_enc = pad_sequences(train_data, padding ='pre',maxlen = MAX_LEN,value =0)\n",
    "X_test_enc = pad_sequences(test_data, padding ='pre',maxlen = MAX_LEN,value =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a padded review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  1 30 31 32  3 33 34 35 36 37 38 39]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_enc[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 3000 samples, we will split the data as follows: 600 for validation and the rest for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.array(X_train_enc[:600])\n",
    "partial_X_train = np.array(X_train_enc[600:])\n",
    "\n",
    "y_val = np.array(train_labels[:600])\n",
    "partial_y_train = np.array(train_labels[600:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode a tweet (index 6 we have been using so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> عاص حلان ضيح ابل حيا اولاد ادفع مقابل اثار مقال نشر'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = X_val[6]\n",
    "' '.join([idx2word.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 24, 32)            224032    \n",
      "_________________________________________________________________\n",
      "LSTM_Layer (LSTM)            (None, 24)                5472      \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 229,579\n",
      "Trainable params: 229,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# first layer = randomly initialized embedding layer\n",
    "#second layer LSTM with 100 units and a linear activation\n",
    "#last layer = output layer.choose softmax classification for multiple classes\n",
    "# Compile model: binary_crossentropy, adam optimizer, metrics as accuracy\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, LSTM, Dense, Input\n",
    "\n",
    "EMBED_SIZE = 32\n",
    "\n",
    "main_input = Input(shape=(MAX_LEN,), name='input_1')\n",
    "layer1= Embedding(VOCAB_SIZE+1,output_dim=EMBED_SIZE, name='Embedding_Layer')(main_input)\n",
    "layer2 = LSTM(MAX_LEN, name='LSTM_Layer')(layer1)\n",
    "output = Dense(3, activation = \"softmax\", name='Output_Layer')(layer2) \n",
    "model = Model(inputs=main_input, outputs=output)\n",
    "\n",
    "model.compile(optimizer = \"adam\",loss = \"sparse_categorical_crossentropy\",metrics = [\"accuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"304pt\" viewBox=\"0.00 0.00 352.00 304.00\" width=\"352pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 348,-300 348,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2525405264808 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2525405264808</title>\n",
       "<polygon fill=\"none\" points=\"42.5,-249.5 42.5,-295.5 301.5,-295.5 301.5,-249.5 42.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"105.5\" y=\"-268.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"168.5,-249.5 168.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"196.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"168.5,-272.5 224.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"196.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"224.5,-249.5 224.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-280.3\">(None, 24)</text>\n",
       "<polyline fill=\"none\" points=\"224.5,-272.5 301.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-257.3\">(None, 24)</text>\n",
       "</g>\n",
       "<!-- 2525323023248 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2525323023248</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 344,-212.5 344,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95\" y=\"-185.8\">Embedding_Layer: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"190,-166.5 190,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"190,-189.5 246,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"246,-166.5 246,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295\" y=\"-197.3\">(None, 24)</text>\n",
       "<polyline fill=\"none\" points=\"246,-189.5 344,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295\" y=\"-174.3\">(None, 24, 32)</text>\n",
       "</g>\n",
       "<!-- 2525405264808&#45;&gt;2525323023248 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2525405264808-&gt;2525323023248</title>\n",
       "<path d=\"M172,-249.366C172,-241.152 172,-231.658 172,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"175.5,-222.607 172,-212.607 168.5,-222.607 175.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2525405262344 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2525405262344</title>\n",
       "<polygon fill=\"none\" points=\"26.5,-83.5 26.5,-129.5 317.5,-129.5 317.5,-83.5 26.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95\" y=\"-102.8\">LSTM_Layer: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"163.5,-83.5 163.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"163.5,-106.5 219.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"219.5,-83.5 219.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268.5\" y=\"-114.3\">(None, 24, 32)</text>\n",
       "<polyline fill=\"none\" points=\"219.5,-106.5 317.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268.5\" y=\"-91.3\">(None, 24)</text>\n",
       "</g>\n",
       "<!-- 2525323023248&#45;&gt;2525405262344 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2525323023248-&gt;2525405262344</title>\n",
       "<path d=\"M172,-166.366C172,-158.152 172,-148.658 172,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"175.5,-139.607 172,-129.607 168.5,-139.607 175.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2525405264752 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2525405264752</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-0.5 37.5,-46.5 306.5,-46.5 306.5,-0.5 37.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"105.5\" y=\"-19.8\">Output_Layer: Dense</text>\n",
       "<polyline fill=\"none\" points=\"173.5,-0.5 173.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"173.5,-23.5 229.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-0.5 229.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268\" y=\"-31.3\">(None, 24)</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-23.5 306.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268\" y=\"-8.3\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 2525405262344&#45;&gt;2525405264752 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2525405262344-&gt;2525405264752</title>\n",
       "<path d=\"M172,-83.3664C172,-75.1516 172,-65.6579 172,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"175.5,-56.6068 172,-46.6068 168.5,-56.6069 175.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils import vis_utils\n",
    "\n",
    "SVG(vis_utils.model_to_dot(model,show_shapes=True, show_layer_names=True).create(prog='dot',format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with different batch sizes and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2506 samples, validate on 600 samples\n",
      "Epoch 1/3\n",
      "2506/2506 [==============================] - 5s 2ms/step - loss: 1.0812 - acc: 0.4362 - val_loss: 1.0861 - val_acc: 0.3450\n",
      "Epoch 2/3\n",
      "2506/2506 [==============================] - 2s 817us/step - loss: 1.0425 - acc: 0.4433 - val_loss: 1.0677 - val_acc: 0.3550\n",
      "Epoch 3/3\n",
      "2506/2506 [==============================] - 2s 823us/step - loss: 0.9837 - acc: 0.5315 - val_loss: 1.0699 - val_acc: 0.4367\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs =3,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluating the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5513/5513 [==============================] - 2s 312us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test_enc, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.065159066714199 test_accuracy: 0.4598222383511341\n"
     ]
    }
   ],
   "source": [
    "print('test_loss:', results[0], 'test_accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Extracting the word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.get_layer('Embedding_Layer').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word_embeddings: (7001, 32)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of word_embeddings:', word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualize the Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0         1         2         3         4         5         6   \\\n",
      "تصوت    -0.028967  0.038130  0.029281  0.003861 -0.043224 -0.043419 -0.023744   \n",
      "زغبي    -0.032110  0.033879 -0.010732  0.029657  0.005036 -0.002264 -0.041344   \n",
      "ديفا     0.024487  0.019183  0.032826 -0.046297  0.023992  0.028768  0.039784   \n",
      "زغب      0.044947  0.034621  0.019366 -0.007501  0.016604  0.043210 -0.022844   \n",
      "نفخ     -0.032621  0.004965 -0.002836 -0.004577 -0.015021  0.042667 -0.014872   \n",
      "صاعد     0.012544 -0.004584 -0.001856 -0.039368  0.008898  0.038536  0.017443   \n",
      "ايدل    -0.014691 -0.028992  0.020912  0.039817 -0.006463 -0.015550  0.043496   \n",
      "النجمة  -0.041935  0.029659 -0.019637 -0.005137  0.013545 -0.047727 -0.029749   \n",
      "الذهبية  0.020374 -0.005478 -0.008712  0.048202 -0.043495 -0.035005  0.001324   \n",
      "توينز    0.008937  0.047563  0.047997 -0.009630 -0.007304  0.042717  0.038370   \n",
      "\n",
      "               7         8         9   ...        22        23        24  \\\n",
      "تصوت     0.000696  0.015004  0.016677  ... -0.049107 -0.039862  0.025076   \n",
      "زغبي    -0.023317  0.042341 -0.049503  ... -0.048869  0.044563  0.026686   \n",
      "ديفا     0.023979  0.001017  0.043654  ... -0.047295  0.004340  0.032806   \n",
      "زغب     -0.041667 -0.033791  0.041869  ... -0.000102  0.018548  0.035770   \n",
      "نفخ     -0.046390  0.039513 -0.009331  ...  0.006982  0.026115 -0.004497   \n",
      "صاعد    -0.014392 -0.010592 -0.045565  ...  0.033135 -0.046682  0.016806   \n",
      "ايدل    -0.002704 -0.028057  0.020283  ... -0.013724 -0.042576  0.035397   \n",
      "النجمة  -0.027389 -0.047380 -0.042346  ...  0.022308 -0.000819  0.040323   \n",
      "الذهبية -0.032088 -0.016511 -0.022737  ... -0.028900 -0.034969  0.026930   \n",
      "توينز   -0.046942  0.020797 -0.021404  ...  0.030518 -0.003625 -0.033862   \n",
      "\n",
      "               25        26        27        28        29        30        31  \n",
      "تصوت     0.036810 -0.034578  0.001183  0.045153  0.019066 -0.023992  0.007796  \n",
      "زغبي     0.041404 -0.011755 -0.019226  0.017988  0.032442  0.008026 -0.026084  \n",
      "ديفا     0.007730 -0.014246 -0.046738  0.001103  0.018066  0.049812 -0.048542  \n",
      "زغب      0.011953  0.044645 -0.026034 -0.003986  0.014107 -0.030444 -0.022497  \n",
      "نفخ      0.034253 -0.040843  0.028761  0.021265  0.024046 -0.014199  0.047876  \n",
      "صاعد     0.004028  0.012666 -0.040496  0.042390  0.014880  0.027059  0.004501  \n",
      "ايدل     0.011386  0.021746  0.044301  0.008222  0.021656 -0.013034 -0.020051  \n",
      "النجمة   0.049126 -0.026529 -0.011651 -0.037028 -0.002565 -0.032980 -0.027366  \n",
      "الذهبية -0.013186  0.040881 -0.048766 -0.033858  0.032013 -0.039920  0.032482  \n",
      "توينز    0.019470  0.025575 -0.044518 -0.013713 -0.011770  0.046638 -0.031487  \n",
      "\n",
      "[10 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "print(DataFrame(word_embeddings, index=idx2word.values()).tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the word embeddings using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "tsne = TSNE(perplexity=3, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# starting from the first word\n",
    "start = 3\n",
    "plot_only = 54\n",
    "T = tsne.fit_transform(word_embeddings[start:plot_only, :])\n",
    "labels = [idx2word[i] for i in range(start, plot_only)]\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1])\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points',ha='right',va='bottom')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
